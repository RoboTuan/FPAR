{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "save_Cams.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPSJYD6/tml1VIzPj0oER06",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoboTuan/FPAR/blob/master/save_Cams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZe5ThxivkJP",
        "outputId": "d5e0b8be-818e-41ed-cc6f-b8dbad244972"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Subset, DataLoader\r\n",
        "from torch.backends import cudnn\r\n",
        "import tensorflow as tf\r\n",
        "import torchvision\r\n",
        "import numpy as np\r\n",
        "from torchvision import transforms\r\n",
        "import cv2\r\n",
        "\r\n",
        "from PIL import Image\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR59gOn7vzhN",
        "outputId": "d81dfe9e-d100-40dc-ab4e-c23f62e62a32"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stjLDwXjv0R4"
      },
      "source": [
        "!unzip -q \"./drive/My Drive/GTEA61.zip\"\r\n",
        "!rm -rf \"./__MACOSX\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBvmuXzOv0ZL"
      },
      "source": [
        "!rm -rf \"./ML_DL_Project\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBfDhQaBv27c",
        "outputId": "4a5f2d80-6dce-444d-8e45-75369550645e"
      },
      "source": [
        "import os \r\n",
        "if not os.path.isdir('./ML_DL_Project'):\r\n",
        "  !git clone https://github.com/RoboTuan/ML_DL_Project.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML_DL_Project'...\n",
            "remote: Enumerating objects: 263, done.\u001b[K\n",
            "remote: Counting objects: 100% (263/263), done.\u001b[K\n",
            "remote: Compressing objects: 100% (252/252), done.\u001b[K\n",
            "remote: Total 793 (delta 179), reused 25 (delta 11), pack-reused 530\u001b[K\n",
            "Receiving objects: 100% (793/793), 92.95 MiB | 22.12 MiB/s, done.\n",
            "Resolving deltas: 100% (534/534), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBteKB23H3OF"
      },
      "source": [
        "from ML_DL_Project.Scripts.objectAttentionModelConvLSTM import *\r\n",
        "\r\n",
        "from ML_DL_Project.Scripts.attentionMapModel import attentionMap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNoQwhA5m8m-",
        "outputId": "fea8dd11-77ff-41e3-884e-334baa853d47"
      },
      "source": [
        "base_dataset = \"./GTEA61\"\n",
        "\n",
        "if not os.path.isdir('./GTEA61_val'):\n",
        "  os.makedirs(\"./GTEA61_val\")\n",
        "  os.makedirs(\"./GTEA61_val/flow_x_processed\")\n",
        "  os.makedirs(\"./GTEA61_val/flow_y_processed\")\n",
        "  os.makedirs(\"./GTEA61_val/processed_frames2\")\n",
        "  pass\n",
        "\n",
        "\n",
        "for directory in sorted(os.listdir(base_dataset)):\n",
        "  if not directory.startswith('.'):\n",
        "    directory_path = os.path.join(base_dataset, directory)\n",
        "    new_directory_path = os.path.join(\"./GTEA61_val\", directory)\n",
        "\n",
        "    for user in sorted(os.listdir(directory_path)):\n",
        "      if not user.startswith('.') and user == \"S2\":\n",
        "        current_folder = os.path.join(directory_path, user)\n",
        "        !cp -r \"{current_folder}\" \"{new_directory_path}\"\n",
        "        !rm -rf \"{current_folder}\"\n",
        "        print(current_folder)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./GTEA61/flow_x_processed/S2\n",
            "./GTEA61/flow_y_processed/S2\n",
            "./GTEA61/processed_frames2/S2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb9JMljOC0XA"
      },
      "source": [
        "!rm -rf 'CAMs'"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um77xNoVzI8F"
      },
      "source": [
        "  if not os.path.isdir('./CAMs'):\r\n",
        "    os.makedirs('./CAMs')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g1jyZTU2J7g"
      },
      "source": [
        "\r\n",
        "def takeError(examinedClass,model_path,model_type,selfsup_bool,regression_bool):\r\n",
        "  stackSize=1\r\n",
        "  Dataset = []\r\n",
        "  Labels = []\r\n",
        "  NumFrames = []\r\n",
        "  directory = examinedClass\r\n",
        "  # print for debugging\r\n",
        "  print(f\"directory: {directory}\")\r\n",
        "  action = directory.split('/')[-1]\r\n",
        "  if not os.path.isdir(f'./CAMs/{model_type}'):\r\n",
        "    os.makedirs(f'./CAMs/{model_type}')\r\n",
        "    os.makedirs(f'./CAMs/{model_type}/' + action)\r\n",
        "  elif not os.path.isdir(f'./CAMs/{model_type}/'+ action):\r\n",
        "    os.makedirs(f'./CAMs/{model_type}/' + action)\r\n",
        "\r\n",
        "  \r\n",
        "  for target in sorted(os.listdir(directory)):\r\n",
        "    #print(f\"target: {target}\")\r\n",
        "    if not target.startswith('.'):\r\n",
        "      #print(target)\r\n",
        "      directory1 = os.path.join(directory, target)\r\n",
        "      # print for debugging\r\n",
        "      #print(f\"directory1: {directory1}\")\r\n",
        "      insts = sorted(os.listdir(directory1))\r\n",
        "      # print for debugging\r\n",
        "      #print(f\"insts: {insts}\")\r\n",
        "      #insts = insts + \"/rgb\"\r\n",
        "      inst_dir= os.path.join(directory1,insts[1])\r\n",
        "      #print(f\"inst_dir {inst_dir}\")\r\n",
        "\r\n",
        "\r\n",
        "      for image in sorted(os.listdir(inst_dir)):\r\n",
        "        image_path= os.path.join(inst_dir, image)\r\n",
        "        #print(f\"image_path :{image_path}\")\r\n",
        "\r\n",
        "        ###################################################################\r\n",
        "        num_classes = 61 # Classes in the pre-trained model\r\n",
        "        mem_size = 512\r\n",
        "        model_state_dict = model_path # Weights of the pre-trained model\r\n",
        "\r\n",
        "        if (selfsup_bool == True):\r\n",
        "          model = SelfSupAttentionModel(num_classes=num_classes, mem_size=mem_size, REGRESSOR=regression_bool)\r\n",
        "        else:\r\n",
        "          model = attentionModel(num_classes=num_classes, mem_size=mem_size)\r\n",
        "        model.load_state_dict(torch.load(model_state_dict))\r\n",
        "        model_backbone = model.resNet\r\n",
        "        attentionMapModel = attentionMap(model_backbone).cuda()\r\n",
        "        attentionMapModel.train(False)\r\n",
        "        for params in attentionMapModel.parameters():\r\n",
        "            params.requires_grad = False\r\n",
        "        ###################################################################\r\n",
        "\r\n",
        "        normalize = transforms.Normalize(\r\n",
        "          mean=[0.485, 0.456, 0.406],\r\n",
        "          std=[0.229, 0.224, 0.225]\r\n",
        "        )\r\n",
        "        preprocess1 = transforms.Compose([\r\n",
        "          transforms.Scale(256),\r\n",
        "          transforms.CenterCrop(224),\r\n",
        "        ])\r\n",
        "\r\n",
        "        preprocess2 = transforms.Compose([\r\n",
        "            transforms.ToTensor(),\r\n",
        "            normalize])\r\n",
        "\r\n",
        "\r\n",
        "        fl_name_in = image_path\r\n",
        "        #print(f\"fl_name_in :{fl_name_in}\")\r\n",
        "        fl_name_out = \"./CAMs/{}/{}/{}_{}.jpg\".format(model_type,action, str(target),fl_name_in.split('/')[-1].split('.')[0])\r\n",
        "        #print(f\"fl_name_out :{fl_name_out}\")\r\n",
        "        img_pil = Image.open(fl_name_in)\r\n",
        "        img_pil1 = preprocess1(img_pil)\r\n",
        "        img_size = img_pil1.size\r\n",
        "        size_upsample = (img_size[0], img_size[1])\r\n",
        "        img_tensor = preprocess2(img_pil1)\r\n",
        "        img_variable = Variable(img_tensor.unsqueeze(0).cuda())\r\n",
        "        img = np.asarray(img_pil1)\r\n",
        "        attentionMap_image = attentionMapModel(img_variable, img, size_upsample)\r\n",
        "        cv2.imwrite(fl_name_out, attentionMap_image)\r\n",
        "\r\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaHXoYv5x8iP"
      },
      "source": [
        "## RGB MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Mw9yqRxd_R",
        "outputId": "d81e6a3b-3bbf-4948-8ad4-8f04a855aeff"
      },
      "source": [
        "\r\n",
        "!unzip -q \"./drive/MyDrive/Runs/Parte 1/7FramesRGB_Attention.zip\"\r\n",
        "!rm -rf \"./__MACOSX\""
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open ./drive/MyDrive/Runs/Parte 1/7FramesRGB_Attention.zip, ./drive/MyDrive/Runs/Parte 1/7FramesRGB_Attention.zip.zip or ./drive/MyDrive/Runs/Parte 1/7FramesRGB_Attention.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miG7pq3dytkX"
      },
      "source": [
        "!rm -rf \"./CAMs/RGB_7\""
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geyQlr34DgD6",
        "outputId": "36c34bfd-9420-44b8-b03a-4fd5bbac731f"
      },
      "source": [
        "takeError(\"./GTEA61_val/processed_frames2/S2/pour_chocolate,bread\",model_path='./7FramesRGB_Attention/model_rgb_state_dict(1).pth',model_type='RGB_7',\r\n",
        "          selfsup_bool=False,regression_bool=False) #save cams of one of the never predicted class\r\n",
        "takeError(\"./GTEA61_val/processed_frames2/S2/open_chocolate\",model_path='./7FramesRGB_Attention/model_rgb_state_dict(1).pth',model_type='RGB_7',\r\n",
        "          selfsup_bool=False,regression_bool=False) #save cams of the class that is always selected instead of the previous one"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "directory: ./GTEA61_val/processed_frames2/S2/pour_chocolate,bread\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "directory: ./GTEA61_val/processed_frames2/S2/open_chocolate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDuE-ghgyGhO"
      },
      "source": [
        "##RGB MODEL 16 FRAMES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QblrbcFgyLVz",
        "outputId": "63dc1466-e7d0-4c21-f091-c02f45eea1b6"
      },
      "source": [
        "!unzip -q \"./drive/MyDrive/Runs/Parte 1/16FramesRGB_Attention.zip\" \r\n",
        "!rm -rf \"./__MACOSX\""
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace 16FramesRGB_Attention/close_honey_rgb_attention1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41R7Xytc2GeX",
        "outputId": "17779ee0-36fc-45f9-de63-75998a953e70"
      },
      "source": [
        "takeError(\"./GTEA61_val/processed_frames2/S2/pour_chocolate,bread\",model_path='./16FramesRGB_Attention/model_rgb_state_dict (1).pth',model_type='RGB_16',\r\n",
        "          selfsup_bool=False,regression_bool=False) #save cams of one of the never predicted class\r\n",
        "takeError(\"./GTEA61_val/processed_frames2/S2/open_chocolate\",model_path='./16FramesRGB_Attention/model_rgb_state_dict (1).pth',model_type='RGB_16',\r\n",
        "          selfsup_bool=False,regression_bool=False) #save cams of the class that is always selected instead of the previous one-"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "directory: ./GTEA61_val/processed_frames2/S2/pour_chocolate,bread\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "directory: ./GTEA61_val/processed_frames2/S2/open_chocolate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ELNhj-jyDOi"
      },
      "source": [
        "## SELF SUP MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AeY39rH72ZV"
      },
      "source": [
        "from ML_DL_Project.Scripts.SelfSupObjectAttentionModelConvLSTM import *"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLbA9IWBxdpj",
        "outputId": "695f397d-1269-484a-afe3-6a683340a076"
      },
      "source": [
        "!unzip -q \"./drive/MyDrive/Runs/Parte 2/No Regression/Decay rate optimization/cartella_selfSupDecayRate0,5_prova2.zip\"\r\n",
        "!rm -rf \"./__MACOSX\""
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace outDir/gtea61/selfSup/stage2/events.out.tfevents.1608580867.a8826bb2cf35.57.2? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-u4zMJx24YX",
        "outputId": "232bed97-a18f-412f-ed28-13ccbd49a478"
      },
      "source": [
        "takeError(\"./GTEA61_val/processed_frames2/S2/pour_chocolate,bread\",model_path='./outDir/gtea61/selfSup/stage2/model_selfSup_state_dict.pth',model_type='self_sup',\r\n",
        "          selfsup_bool=True,regression_bool=False) #save cams of one of the never predicted class\r\n",
        "takeError(\"./GTEA61_val/processed_frames2/S2/open_chocolate\",model_path='./outDir/gtea61/selfSup/stage2/model_selfSup_state_dict.pth',model_type='self_sup',\r\n",
        "          selfsup_bool=True,regression_bool=False) #save cams of the class that is always selected instead of the previous one-"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "directory: ./GTEA61_val/processed_frames2/S2/pour_chocolate,bread\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "directory: ./GTEA61_val/processed_frames2/S2/open_chocolate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7xA7kD30E3g"
      },
      "source": [
        "## SELF SUP REGRESSION MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLGmbNM973pd"
      },
      "source": [
        "from ML_DL_Project.Scripts.SelfSupObjectAttentionModelConvLSTM import *"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS1efWUt0JZW",
        "outputId": "d823a8b7-aaff-4e8a-86b9-d34ee4d3bdf2"
      },
      "source": [
        "!unzip -q \"./drive/MyDrive/Runs/Parte 2/Regression/Decay rate optimization/cartella_RegselfSupGirodecayRate0.3_prova1.zip\" \r\n",
        "!rm -rf \"./__MACOSX\""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace cartella_RegselfSupGirodecayRate0.3_prova1/mmap_train_loss.svg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fb4lYyE5P3f",
        "outputId": "0b2a8af3-ec03-49a0-c4a1-d3dc8d0b9760"
      },
      "source": [
        "takeError(\"./GTEA61_val/processed_frames2/S2/pour_chocolate,bread\",model_path='./16FramesRGB_Attention/model_rgb_state_dict (1).pth',model_type='self_sup_regr',\r\n",
        "          selfsup_bool=True,regression_bool=True) #save cams of one of the never predicted class\r\n",
        "takeError(\"./GTEA61_val/processed_frames2/S2/open_chocolate\",model_path='./16FramesRGB_Attention/model_rgb_state_dict (1).pth',model_type='self_sup_regr',\r\n",
        "          selfsup_bool=True,regression_bool=True) #save cams of the class that is always selected instead of the previous one-"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "directory: ./GTEA61_val/processed_frames2/S2/scoop_coffee,spoon\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "directory: ./GTEA61_val/processed_frames2/S2/take_coffee\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}