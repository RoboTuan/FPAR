{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self-supervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNYeteZDjbPRQoVpCM/L2P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoboTuan/ML_DL_Project/blob/master/Self_supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYoVb2SQnG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6888c003-f4d0-47a6-f1c3-543b52eadf2d"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.autograd import Function\n",
        "import torchvision\n",
        "from torchvision.models import alexnet\n",
        "from torchvision.utils import make_grid\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import pandas.util.testing as tm\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZVmFAwkQ0AS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "49d0c8a6-b1da-418e-81cf-ebbfc6966999"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzqpeOqKQ0Vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"./drive/My Drive/Copia di GTEA61.zip\"\n",
        "!rm -rf \"./__MACOSX\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihz9gTQEQ0Z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf \"./ML_DL_Project/\"\n",
        "#!rm -rf \"./out_dir/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKvNku7pQ0cp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ef0d94c2-19cf-4254-b082-7a757a091443"
      },
      "source": [
        "import os \n",
        "if not os.path.isdir('./ML_DL_Project'):\n",
        "  !git clone https://github.com/RoboTuan/ML_DL_Project.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML_DL_Project'...\n",
            "remote: Enumerating objects: 204, done.\u001b[K\n",
            "remote: Counting objects: 100% (204/204), done.\u001b[K\n",
            "remote: Compressing objects: 100% (166/166), done.\u001b[K\n",
            "remote: Total 204 (delta 127), reused 78 (delta 38), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (204/204), 4.32 MiB | 4.01 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-6YG3acQ0g3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c4a9bd27-cd56-451b-817e-61e3bd593c7a"
      },
      "source": [
        "base_dataset = \"./GTEA61\"\n",
        "\n",
        "if not os.path.isdir('./GTEA61_val'):\n",
        "  os.makedirs(\"./GTEA61_val\")\n",
        "  os.makedirs(\"./GTEA61_val/flow_x_processed\")\n",
        "  os.makedirs(\"./GTEA61_val/flow_y_processed\")\n",
        "  os.makedirs(\"./GTEA61_val/processed_frames2\")\n",
        "  pass\n",
        "\n",
        "\n",
        "for directory in sorted(os.listdir(base_dataset)):\n",
        "  if not directory.startswith('.'):\n",
        "    directory_path = os.path.join(base_dataset, directory)\n",
        "    new_directory_path = os.path.join(\"./GTEA61_val\", directory)\n",
        "\n",
        "    for user in sorted(os.listdir(directory_path)):\n",
        "      if not user.startswith('.') and user == \"S2\":\n",
        "        current_folder = os.path.join(directory_path, user)\n",
        "        !cp -r \"{current_folder}\" \"{new_directory_path}\"\n",
        "        !rm -rf \"{current_folder}\"\n",
        "        print(current_folder)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./GTEA61/flow_x_processed/S2\n",
            "./GTEA61/flow_y_processed/S2\n",
            "./GTEA61/processed_frames2/S2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nepLRofOQ0ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se scritti uguali ai parametri della funzione può venire None come risultato\n",
        "# per questo SEQLEN è maiuscolo \n",
        "SEQLEN = 7\n",
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = 61\n",
        "#STAGE = None\n",
        "train_data_dir = \"./GTEA61\"\n",
        "val_data_dir = \"./GTEA61_val\"\n",
        "#stage1_dict = None\n",
        "out_dir = \".out_dir\"\n",
        "trainBatchSize = 32\n",
        "valBatchSize = 64\n",
        "numEpochs1 = 3\n",
        "numEpochs2 = 3\n",
        "lr1 = 1e-3\n",
        "lr2 = 1e-4\n",
        "decay_factor = 0.1\n",
        "decay_step1 = [25, 75, 150]\n",
        "decay_step2 = [25, 75]\n",
        "#Boh?!\n",
        "MEMSIZE = 512"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZoYPO1hQ0o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf \"ML_DL_Project\"\n",
        "#!rm -rf \"./GTEA61\"\n",
        "#!rm -rf \"./GTEA61_val\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrGDU3ojQ0uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "# This is without attention, we must address this better\n",
        "#from ML_DL_Project.Scripts.convLSTMmodel import *\n",
        "from ML_DL_Project.Scripts.objectAttentionModelConvLSTM import *\n",
        "from ML_DL_Project.Scripts.spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
        "                                RandomHorizontalFlip)\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "#from ML_DL_Project.Scripts.makeDatasetRGB import *\n",
        "from ML_DL_Project.Scripts.makeMmaps import *\n",
        "import argparse\n",
        "import sys"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek8DDCr0Q0tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dir for saving models and log files\n",
        "model_folder = os.path.join('./', \"out_dir\", \"./GTEA61\", 'rgb')\n",
        "\n",
        "if not os.path.isdir('./out_dir'):\n",
        "  if os.path.exists(model_folder):\n",
        "    print('Directory {} exists!'.format(model_folder))\n",
        "    sys.exit()\n",
        "    \n",
        "  os.makedirs(model_folder)\n",
        "\n",
        "writer = SummaryWriter(model_folder)\n",
        "train_log_loss = open((model_folder + '/train_log_loss.txt'), 'w')\n",
        "train_log_acc = open((model_folder + '/train_log_acc.txt'), 'w')\n",
        "val_log_loss = open((model_folder + '/val_log_loss.txt'), 'w')\n",
        "val_log_acc = open((model_folder + '/val_log_acc.txt'), 'w')\n",
        "train_log_loss_ss = open((model_folder + '/train_log_loss_ss.txt'), 'w')\n",
        "train_log_acc_ss = open((model_folder + '/train_log_acc_ss.txt'), 'w')\n",
        "val_log_loss_ss = open((model_folder + '/val_log_loss_ss.txt'), 'w')\n",
        "val_log_acc_ss = open((model_folder + '/val_log_acc_ss.txt'), 'w')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkJnSxGAS_oD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data loader\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "spatial_transform = Compose([Scale(256),\n",
        "                             RandomHorizontalFlip(), \n",
        "                             MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                             ToTensor(), \n",
        "                             normalize])\n",
        "\n",
        "vid_seq_train = makeDataset(train_data_dir, spatial_transform=spatial_transform, seqLen=SEQLEN, fmt='.png')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(vid_seq_train, batch_size=trainBatchSize, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "if val_data_dir is not None:\n",
        "  vid_seq_val = makeDataset(val_data_dir,spatial_transform = Compose([Scale(256),\n",
        "                                                                    CenterCrop(224),\n",
        "                                                                    ToTensor(),\n",
        "                                                                    normalize]),\n",
        "                            seqLen=SEQLEN, fmt='.png')\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(vid_seq_val, batch_size=valBatchSize, shuffle=False, num_workers=2, pin_memory=True)\n",
        "  valInstances = vid_seq_val.__len__()\n",
        "\n",
        "\n",
        "trainInstances = vid_seq_train.__len__()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYj-6fdmS_s3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "spatial_transform = Compose([Scale(256),\n",
        "                             RandomHorizontalFlip(), \n",
        "                             MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                             ToTensor(), \n",
        "                             normalize])\n",
        "\n",
        "vid_seq_train = makeDataset(train_data_dir, spatial_transform=spatial_transform, seqLen=SEQLEN, fmt='.png')\n",
        "vid_seq_val = makeDataset(val_data_dir, spatial_transform = Compose([Scale(256),\n",
        "                                                                    CenterCrop(224),\n",
        "                                                                    ToTensor(),\n",
        "                                                                    normalize]),\n",
        "                          seqLen=SEQLEN, fmt='.png')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWfJ-uzLS_zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(stage, learning_rate, numEpochs, decay_step, regressor, stage1_dict=None):\n",
        "  train_params = []\n",
        "  if stage == 1:\n",
        "    model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEMSIZE)\n",
        "    model.train(False)\n",
        "    for params in model.parameters():\n",
        "        params.requires_grad = False\n",
        "  else:\n",
        "    model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEMSIZE)\n",
        "    model.load_state_dict(torch.load(stage1_dict))\n",
        "    model.train(False)\n",
        "\n",
        "    for params in model.parameters():\n",
        "          params.requires_grad = False\n",
        "      #\n",
        "    for params in model.resNet.layer4[0].conv1.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.resNet.layer4[0].conv2.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.resNet.layer4[1].conv1.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.resNet.layer4[1].conv2.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.resNet.layer4[2].conv1.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "      #\n",
        "    for params in model.resNet.layer4[2].conv2.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "      #\n",
        "    for params in model.resNet.fc.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    model.resNet.layer4[0].conv1.train(True)\n",
        "    model.resNet.layer4[0].conv2.train(True)\n",
        "    model.resNet.layer4[1].conv1.train(True)\n",
        "    model.resNet.layer4[1].conv2.train(True)\n",
        "    model.resNet.layer4[2].conv1.train(True)\n",
        "    model.resNet.layer4[2].conv2.train(True)\n",
        "    model.resNet.fc.train(True)\n",
        "\n",
        "  for params in model.lstm_cell.parameters():\n",
        "      params.requires_grad = True\n",
        "      train_params += [params]\n",
        "\n",
        "  for params in model.classifier.parameters():\n",
        "      params.requires_grad = True\n",
        "      train_params += [params]\n",
        "  model.lstm_cell.train(True)\n",
        "\n",
        "  model.classifier.train(True)\n",
        "\n",
        "  model = model.to(DEVICE)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  loss_regression= nn.MSELoss()\n",
        "  loss_ms=nn.NLLLoss()\n",
        "\n",
        "  optimizer_fn = torch.optim.Adam(train_params, lr=learning_rate, weight_decay=4e-5, eps=1e-4)\n",
        "\n",
        "  optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=decay_step, gamma=decay_factor)\n",
        "  train_iter = 0\n",
        "  min_accuracy = 0\n",
        "\n",
        "  for epoch in range(numEpochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_loss_ss = 0\n",
        "    numCorrTrain = 0\n",
        "    numCorrTrain_ss = 0\n",
        "    trainSamples = 0\n",
        "    iterPerEpoch = 0\n",
        "    model.lstm_cell.train(True)\n",
        "    model.classifier.train(True)\n",
        "    writer.add_scalar('lr', optimizer_fn.param_groups[0]['lr'], epoch+1)\n",
        "\n",
        "    if stage == 2:\n",
        "        model.resNet.layer4[0].conv1.train(True)\n",
        "        model.resNet.layer4[0].conv2.train(True)\n",
        "        model.resNet.layer4[1].conv1.train(True)\n",
        "        model.resNet.layer4[1].conv2.train(True)\n",
        "        model.resNet.layer4[2].conv1.train(True)\n",
        "        model.resNet.layer4[2].conv2.train(True)\n",
        "        model.resNet.fc.train(True)\n",
        "    for i, (inputs, maps, targets) in enumerate(train_loader):\n",
        "      train_iter += 1\n",
        "      iterPerEpoch += 1\n",
        "      optimizer_fn.zero_grad()\n",
        "      inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).to(DEVICE))\n",
        "      labelVariable = Variable(targets.to(DEVICE))\n",
        "      trainSamples += inputs.size(0)\n",
        "      output_label, output_ss= model(inputVariable)\n",
        "      loss = loss_fn(output_label, labelVariable)\n",
        "      loss.backward()\n",
        "      if regressor==0:\n",
        "        maps=Variable(maps.permute(1, 0, 2, 3, 4).type(torch.LongTensor).to(DEVICE))\n",
        "        output_ss= output_ss.view(-1, 2)\n",
        "      if regressor==1:\n",
        "        maps=Variable(maps.permute(1, 0, 2, 3, 4).to(DEVICE))\n",
        "        output_ss=output_ss.view(-1)\n",
        "      maps=maps.contiguos().view(-1)\n",
        "      #print(maps)\n",
        "      #Print for debugging\n",
        "\n",
        "      if regressor==1:\n",
        "        #Yes regression, the loss is obtained from the MSE error\n",
        "        loss_ms=loss_regression(output_ss, maps)\n",
        "        loss_ms.backward()\n",
        "      if regressor==0:\n",
        "        #No regression, the loss is computed by the cross entropy\n",
        "        loss_ms=loss_fn(output_ss, maps)\n",
        "        loss_ms.backward()\n",
        "        _, predicted = torch.max(output_ms.data, 1)\n",
        "        numCorrTrain_ss += torch.sum(predicted == maps.data).data.item()\n",
        "        epoch_loss_ss+=loss_ms.item()\n",
        "      optimizer_fn.step()\n",
        "      _, predicted = torch.max(output_label.data, 1)\n",
        "      numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
        "      epoch_loss+=loss.item()\n",
        "      avg_loss = epoch_loss/iterPerEpoch\n",
        "\n",
        "      if val_data_dir is not None:\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            model.train(False)\n",
        "            val_loss_epoch = 0\n",
        "            val_loss_epoch_ss = 0\n",
        "            val_iter = 0\n",
        "            val_samples = 0\n",
        "            numCorr = 0\n",
        "            numCorr_ss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "              for j, (inputs, maps, targets) in enumerate(val_loader):\n",
        "\n",
        "                  val_iter += 1\n",
        "                  val_samples += inputs.size(0)\n",
        "                  inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "                  labelVariable = targets.to(DEVICE)\n",
        "                  output_label, output_ss = model(inputVariable)\n",
        "                  val_loss = loss_fn(output_label, labelVariable)\n",
        "                  val_loss_epoch += val_loss.item()\n",
        "\n",
        "                  if regressor==0:\n",
        "                    maps=Variable(maps.permute(1, 0, 2, 3, 4).type(torch.LongTensor).to(DEVICE))\n",
        "                    output_ss= output_ss.view(-1, 2)\n",
        "                  if regressor==1:\n",
        "                    maps=Variable(maps.permute(1, 0, 2, 3, 4).to(DEVICE))\n",
        "                    output_ss=output_ss.view(-1)\n",
        "                  maps=maps.contiguos().view(-1)\n",
        "                  if regressor==1:\n",
        "                    #Yes regression, the loss is obtained from the MSE error\n",
        "                    loss_ms=loss_regression(output_ss, maps)\n",
        "                    loss_ms.backward()\n",
        "                  elif regressor==0:\n",
        "                    #No regression, the loss is computed by the cross entropy\n",
        "                    loss_ms=loss_fn(output_ss, maps)\n",
        "                    loss_ms.backward()\n",
        "                    _, predicted = torch.max(output_ms.data, 1)\n",
        "                    numCorr_ss += torch.sum(predicted == maps.data).data.item()\n",
        "                    epoch_loss_ss+=loss_ms.item()\n",
        "\n",
        "                  _, predicted = torch.max(output_label.data, 1)\n",
        "                  numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
        "\n",
        "              avg_val_loss = val_loss_epoch / val_iter\n",
        "              if stage ==2:\n",
        "                    avg_loss_ms= epoch_loss_ss_val/ val_iter\n",
        "                    val_accuracy = (numCorr_ss / val_samples) * 100\n",
        "                    #avg_loss = avg_loss + avg_loss_ms \n",
        "                    #val_log_loss_ms.write('Val Loss MS after {} epochs = {}\\n'.format(epoch + 1, avg_loss_ms))\n",
        "                    if regressor == 0:\n",
        "                      val_log_acc_ms.write('Val Accuracy after {} epochs = {}%\\n'.format(epoch + 1, val_accuracy))\n",
        "              val_accuracy = (numCorr / val_samples) * 100\n",
        "              print('Val: Epoch = {} | Loss {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "\n",
        "              if val_accuracy > min_accuracy:\n",
        "                save_path_model = (model_folder + '/model_rgb_state_dict.pth')\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                min_accuracy = val_accuracy\n",
        "              elif (epoch+1) % 10 == 0:\n",
        "                save_path_model = (model_folder + '/model_rgb_state_dict_epoch' + str(epoch+1) + '.pth')\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "\n",
        "\n",
        "                  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oKpYOA-e_EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Demo for contiguos\n",
        "#aaa = torch.Tensor( [[1,2,3],[4,5,6]] )\n",
        "#print(aaa.stride())\n",
        "#print(aaa.is_contiguous())\n",
        "#bbb = aaa.transpose(0,1)\n",
        "#print(bbb)\n",
        "#print(bbb.is_contiguous())\n",
        "#bbb.contiguous().view(-1,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUyVtMMIa3up",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d626fac6-c3e7-4fe2-b5fa-9fa7297cbc6c"
      },
      "source": [
        "#Demo for view reshaping\n",
        "#x = torch.arange(6)\n",
        "#print(x.view(-1,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tY0fBQXS_2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Self-supervised without regressor\n",
        "train(1, lr1, numEpochs1, 0, decay_step1)\n",
        "\n",
        "train_log_loss.close()\n",
        "train_log_acc.close()\n",
        "val_log_acc.close()\n",
        "val_log_loss.close()\n",
        "#writer.export_scalars_to_json(model_folder + \"/all_scalars.json\")\n",
        "writer.flush()\n",
        "writer.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWH-Jyy27nW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Self supervised with regressor\n",
        "writer = SummaryWriter('runs/exp1/stage/2')\n",
        "train_log_loss = open((model_folder + '/train_log_loss.txt'), 'w')\n",
        "train_log_acc = open((model_folder + '/train_log_acc.txt'), 'w')\n",
        "val_log_loss = open((model_folder + '/val_log_loss.txt'), 'w')\n",
        "val_log_acc = open((model_folder + '/val_log_acc.txt'), 'w')\n",
        "\n",
        "\n",
        "train(2, lr2, numEpochs2, decay_step2, 1, stage1_dict=\"./out_dir/GTEA61/rgb/model_rgb_state_dict.pth\")\n",
        "\n",
        "train_log_loss.close()\n",
        "train_log_acc.close()\n",
        "val_log_acc.close()\n",
        "val_log_loss.close()\n",
        "#writer.export_scalars_to_json(model_folder + \"/all_scalars.json\")\n",
        "writer.flush()\n",
        "writer.close()\n",
        "\n",
        "%tensorboard --logdir=runs/exp1\n",
        "#!rm -rf ./runs/exp1/stage\n",
        "#Using it only if run again, in view to clean the folders"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}