{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prova1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvAmqJ5sJaErfdFX3oSjea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoboTuan/ML_DL_Project/blob/master/prova1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yTB2OrLZRO6",
        "colab_type": "code",
        "outputId": "e1027d1b-d11e-48c4-d2a2-f73e563b8a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X63WTCEnd2EZ",
        "colab_type": "code",
        "outputId": "cc81b480-3477-4200-b659-73a33713cd7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import os \n",
        "if not os.path.isdir('./ML_DL_Project'):\n",
        "  !git clone https://github.com/RoboTuan/ML_DL_Project.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML_DL_Project'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 94 (delta 56), reused 51 (delta 25), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (94/94), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MFvh9AgaoV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se scritti uguali ai parametri della funzione può venire None come risultato\n",
        "# per questo SEQLEN è maiuscolo \n",
        "SEQLEN = 7\n",
        "NUM_CLASSES = 61\n",
        "dataset = None\n",
        "STAGE = None\n",
        "train_data_dir = \"./drive/My Drive/GTEA61\"\n",
        "val_data_dir = None\n",
        "stage1_dict = None\n",
        "out_dir = None\n",
        "seqLen = None\n",
        "trainBatchSize = 32\n",
        "valBatchSize = None\n",
        "numEpochs = 3\n",
        "lr1 = 1e-3\n",
        "decay_factor = None\n",
        "decay_step = None\n",
        "#Boh?!\n",
        "MEMSIZE = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_7vigFWapHx",
        "colab_type": "code",
        "outputId": "90e8b29e-b0bd-4195-ba24-706c791e9f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.autograd import Function\n",
        "import torchvision\n",
        "from torchvision.models import alexnet\n",
        "from torchvision.utils import make_grid\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import pandas.util.testing as tm\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfbDEYGKW8As",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf \"ML_DL_Project\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HObThRuIfHaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from ML_DL_Project.Scripts.objectAttentionModelConvLSTM import *\n",
        "from ML_DL_Project.Scripts.spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
        "                                RandomHorizontalFlip)\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from ML_DL_Project.Scripts.makeDatasetRGB import *\n",
        "import argparse\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5AHTPevarFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dir for saving models and log files\n",
        "model_folder = os.path.join('./', \"out_dir\", \"./GTEA61\", 'rgb')\n",
        "\n",
        "if not os.path.isdir('./out_dir'):\n",
        "  if os.path.exists(model_folder):\n",
        "    print('Directory {} exists!'.format(model_folder))\n",
        "    sys.exit()\n",
        "    \n",
        "  os.makedirs(model_folder)\n",
        "\n",
        "writer = SummaryWriter(model_folder)\n",
        "train_log_loss = open((model_folder + '/train_log_loss.txt'), 'w')\n",
        "train_log_acc = open((model_folder + '/train_log_acc.txt'), 'w')\n",
        "val_log_loss = open((model_folder + '/val_log_loss.txt'), 'w')\n",
        "val_log_acc = open((model_folder + '/val_log_acc.txt'), 'w')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy42Au7A64pR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# import glob\n",
        "# import random\n",
        "\n",
        "\n",
        "# def gen_split(root_dir, stackSize):\n",
        "#     Dataset = []\n",
        "#     Labels = []\n",
        "#     NumFrames = []\n",
        "#     root_dir = os.path.join(root_dir, 'processed_frames2')\n",
        "#     # print for debugging\n",
        "#     #print(f\"root_dir: {root_dir}\")\n",
        "#     for dir_user in sorted(os.listdir(root_dir)):\n",
        "#       if not dir_user.startswith('.') and dir_user != \"S2\" :\n",
        "#         class_id = 0\n",
        "#         directory = os.path.join(root_dir, dir_user)\n",
        "#         # print for debugging\n",
        "#         #print(f\"directory: {directory}\")\n",
        "#         for target in sorted(os.listdir(directory)):\n",
        "#           if not target.startswith('.'):\n",
        "#             directory1 = os.path.join(directory, target)\n",
        "#             # print for debugging\n",
        "#             #print(f\"directory1: {directory1}\")\n",
        "#             insts = sorted(os.listdir(directory1))\n",
        "#             # print for debugging\n",
        "#             #print(f\"insts: {insts}\")\n",
        "#             if insts != []:\n",
        "#                for inst in insts:\n",
        "#                  if not inst.startswith('.'):\n",
        "#                    # adding \"rgb\" to path becasue after the number there are\n",
        "#                    # both \"rgb\" and \"mmap\" directories\n",
        "#                    inst = inst + \"/rgb\"\n",
        "#                    inst_dir = os.path.join(directory1, inst)\n",
        "#                    # print for debugging\n",
        "#                    #print(f\"inst_dir: {inst_dir}\")\n",
        "#                    numFrames = len(glob.glob1(inst_dir, '*.png'))\n",
        "#                    # print for debugging\n",
        "#                    #print(f\"numFrames: {numFrames}\")\n",
        "#                    if numFrames >= stackSize:\n",
        "#                      Dataset.append(inst_dir)\n",
        "#                      Labels.append(class_id)\n",
        "#                      NumFrames.append(numFrames)\n",
        "#             class_id += 1\n",
        "#     return Dataset, Labels, NumFrames\n",
        "\n",
        "# class makeDataset(Dataset):\n",
        "#     def __init__(self, root_dir, spatial_transform=None, seqLen=20,\n",
        "#                  train=True, mulSeg=False, numSeg=1, fmt='.png'):\n",
        "\n",
        "#         self.images, self.labels, self.numFrames = gen_split(root_dir, 5)\n",
        "#         self.spatial_transform = spatial_transform\n",
        "#         self.train = train\n",
        "#         self.mulSeg = mulSeg\n",
        "#         self.numSeg = numSeg\n",
        "#         self.seqLen = seqLen\n",
        "#         # print for debugging\n",
        "#         #print(self.seqLen)\n",
        "#         self.fmt = fmt\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.images)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         vid_name = self.images[idx]\n",
        "#         label = self.labels[idx]\n",
        "#         numFrame = self.numFrames[idx]\n",
        "#         inpSeq = []\n",
        "#          # For debugging\n",
        "#         #print(numFrame, self.seqLen)\n",
        "#         self.spatial_transform.randomize_parameters()\n",
        "#         for i in np.linspace(1, numFrame, self.seqLen, endpoint=False):\n",
        "#           # Corrected with \"rgb\" instead of \"image_\" and zfill(4) instead of zfill(5)\n",
        "#           fl_name = vid_name + '/' + 'rgb' + str(int(np.floor(i))).zfill(4) + self.fmt\n",
        "#           img = Image.open(fl_name)\n",
        "#           # For debugging\n",
        "#           #print(img)\n",
        "#           inpSeq.append(self.spatial_transform(img.convert('RGB')))\n",
        "#         inpSeq = torch.stack(inpSeq, 0)\n",
        "#         return inpSeq, label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I7ggwuIa2WL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data loader\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "spatial_transform = Compose([Scale(256),\n",
        "                             RandomHorizontalFlip(), \n",
        "                             MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                             ToTensor(), \n",
        "                             normalize])\n",
        "\n",
        "vid_seq_train = makeDataset(train_data_dir, spatial_transform=spatial_transform, seqLen=SEQLEN, fmt='.png')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(vid_seq_train, batch_size=trainBatchSize, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "if val_data_dir is not None:\n",
        "  vid_seq_val = makeDataset(val_data_dir,spatial_transform = Compose([Scale(256),\n",
        "                                                                    CenterCrop(224),\n",
        "                                                                    ToTensor(),\n",
        "                                                                    normalize]),\n",
        "                            seqLen=seqLen, fmt='.png')\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(vid_seq_val, batch_size=valBatchSize, shuffle=False, num_workers=2, pin_memory=True)\n",
        "  valInstances = vid_seq_val.__len__()\n",
        "\n",
        "\n",
        "trainInstances = vid_seq_train.__len__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffrv_iq2U6Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STAGE = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ8JCAg5TYxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_params = []\n",
        "if STAGE == 1:\n",
        "\n",
        "    model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEMSIZE)\n",
        "    model.train(False)\n",
        "    for params in model.parameters():\n",
        "        params.requires_grad = False\n",
        "else:\n",
        "\n",
        "    model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEMSIZE)\n",
        "    model.load_state_dict(torch.load(stage1_dict))\n",
        "    model.train(False)\n",
        "    for params in model.parameters():\n",
        "        params.requires_grad = False\n",
        "    #\n",
        "    for params in model.resNet.layer4[0].conv1.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.resNet.layer4[0].conv2.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.resNet.layer4[1].conv1.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.resNet.layer4[1].conv2.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.resNet.layer4[2].conv1.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "    #\n",
        "    for params in model.resNet.layer4[2].conv2.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "    #\n",
        "    for params in model.resNet.fc.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    model.resNet.layer4[0].conv1.train(True)\n",
        "    model.resNet.layer4[0].conv2.train(True)\n",
        "    model.resNet.layer4[1].conv1.train(True)\n",
        "    model.resNet.layer4[1].conv2.train(True)\n",
        "    model.resNet.layer4[2].conv1.train(True)\n",
        "    model.resNet.layer4[2].conv2.train(True)\n",
        "    model.resNet.fc.train(True)\n",
        "\n",
        "for params in model.lstm_cell.parameters():\n",
        "    params.requires_grad = True\n",
        "    train_params += [params]\n",
        "\n",
        "for params in model.classifier.parameters():\n",
        "    params.requires_grad = True\n",
        "    train_params += [params]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y7F_OiSU7a4",
        "colab_type": "code",
        "outputId": "e4061eea-b9b2-44af-e729-faf72891193d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "source": [
        "model.lstm_cell.train(True)\n",
        "\n",
        "model.classifier.train(True)\n",
        "model.cuda()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_fn = torch.optim.Adam(train_params, lr=lr1, weight_decay=4e-5, eps=1e-4)\n",
        "\n",
        "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=decay_step, gamma=decay_factor)\n",
        "\n",
        "train_iter = 0\n",
        "min_accuracy = 0\n",
        "\n",
        "\n",
        "for epoch in range(numEpochs):\n",
        "    optim_scheduler.step()\n",
        "    epoch_loss = 0\n",
        "    numCorrTrain = 0\n",
        "    trainSamples = 0\n",
        "    iterPerEpoch = 0\n",
        "    model.lstm_cell.train(True)\n",
        "    model.classifier.train(True)\n",
        "    writer.add_scalar('lr', optimizer_fn.param_groups[0]['lr'], epoch+1)\n",
        "    if STAGE == 2:\n",
        "        model.resNet.layer4[0].conv1.train(True)\n",
        "        model.resNet.layer4[0].conv2.train(True)\n",
        "        model.resNet.layer4[1].conv1.train(True)\n",
        "        model.resNet.layer4[1].conv2.train(True)\n",
        "        model.resNet.layer4[2].conv1.train(True)\n",
        "        model.resNet.layer4[2].conv2.train(True)\n",
        "        model.resNet.fc.train(True)\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        train_iter += 1\n",
        "        iterPerEpoch += 1\n",
        "        optimizer_fn.zero_grad()\n",
        "        inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).cuda())\n",
        "        labelVariable = Variable(targets.cuda())\n",
        "        trainSamples += inputs.size(0)\n",
        "        output_label, _ = model(inputVariable)\n",
        "        loss = loss_fn(output_label, labelVariable)\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "        _, predicted = torch.max(output_label.data, 1)\n",
        "        numCorrTrain += (predicted == targets.cuda()).sum()\n",
        "        epoch_loss += loss.data[0]\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
        "\n",
        "    print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch+1, avg_loss, trainAccuracy))\n",
        "    writer.add_scalar('train/epoch_loss', avg_loss, epoch+1)\n",
        "    writer.add_scalar('train/accuracy', trainAccuracy, epoch+1)\n",
        "    if val_data_dir is not None:\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            model.train(False)\n",
        "            val_loss_epoch = 0\n",
        "            val_iter = 0\n",
        "            val_samples = 0\n",
        "            numCorr = 0\n",
        "            for j, (inputs, targets) in enumerate(val_loader):\n",
        "                val_iter += 1\n",
        "                val_samples += inputs.size(0)\n",
        "                inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).cuda(), volatile=True)\n",
        "                labelVariable = Variable(targets.cuda(async=True), volatile=True)\n",
        "                output_label, _ = model(inputVariable)\n",
        "                val_loss = loss_fn(output_label, labelVariable)\n",
        "                val_loss_epoch += val_loss.data[0]\n",
        "                _, predicted = torch.max(output_label.data, 1)\n",
        "                numCorr += (predicted == targets.cuda()).sum()\n",
        "            val_accuracy = (numCorr / val_samples) * 100\n",
        "            avg_val_loss = val_loss_epoch / val_iter\n",
        "            print('Val: Epoch = {} | Loss {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "            writer.add_scalar('val/epoch_loss', avg_val_loss, epoch + 1)\n",
        "            writer.add_scalar('val/accuracy', val_accuracy, epoch + 1)\n",
        "            val_log_loss.write('Val Loss after {} epochs = {}\\n'.format(epoch + 1, avg_val_loss))\n",
        "            val_log_acc.write('Val Accuracy after {} epochs = {}%\\n'.format(epoch + 1, val_accuracy))\n",
        "            if val_accuracy > min_accuracy:\n",
        "                save_path_model = (model_folder + '/model_rgb_state_dict.pth')\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                min_accuracy = val_accuracy\n",
        "        else:\n",
        "            if (epoch+1) % 10 == 0:\n",
        "                save_path_model = (model_folder + '/model_rgb_state_dict_epoch' + str(epoch+1) + '.pth')\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "\n",
        "train_log_loss.close()\n",
        "train_log_acc.close()\n",
        "val_log_acc.close()\n",
        "val_log_loss.close()\n",
        "writer.export_scalars_to_json(model_folder + \"/all_scalars.json\")\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fbbec7f9f60>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 50, in wait\n",
            "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 28, in poll\n",
            "    pid, sts = os.waitpid(self.pid, flag)\n",
            "KeyboardInterrupt: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-acf2c1238f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mnumCorrTrain\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0miterPerEpoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtrainAccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumCorrTrain\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrainSamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
          ]
        }
      ]
    }
  ]
}