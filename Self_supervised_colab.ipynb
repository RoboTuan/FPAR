{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self-supervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+/g9oZotx9CZ0xHz5TNek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoboTuan/ML_DL_Project/blob/Codes/Self_supervised_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYoVb2SQnG0",
        "cellView": "both",
        "outputId": "c06d8f4f-676d-4059-b6f3-cbd4ba165bad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.autograd import Function\n",
        "import torchvision\n",
        "from torchvision.models import alexnet\n",
        "from torchvision.utils import make_grid\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import pandas.util.testing as tm\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZVmFAwkQ0AS",
        "outputId": "faeab620-b3f9-4a8f-aa6a-02a6348fc278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzqpeOqKQ0Vt"
      },
      "source": [
        "!unzip -q \"./drive/My Drive/Copia di GTEA61.zip\"\n",
        "!rm -rf \"./__MACOSX\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihz9gTQEQ0Z3"
      },
      "source": [
        "#!rm -rf \"./ML_DL_Project/\"\n",
        "#!rm -rf \"./out_dir/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKvNku7pQ0cp",
        "outputId": "c19ed8ad-3d1d-4d57-9f8e-6d1011f5d276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os \n",
        "if not os.path.isdir('./ML_DL_Project'):\n",
        "  !git clone https://github.com/RoboTuan/ML_DL_Project.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML_DL_Project'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 284 (delta 37), reused 35 (delta 18), pack-reused 230\u001b[K\n",
            "Receiving objects: 100% (284/284), 74.74 MiB | 19.79 MiB/s, done.\n",
            "Resolving deltas: 100% (171/171), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-6YG3acQ0g3",
        "outputId": "909deb4c-c64d-4ff3-ea6c-a860f8634184",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "base_dataset = \"./GTEA61\"\n",
        "\n",
        "if not os.path.isdir('./GTEA61_val'):\n",
        "  os.makedirs(\"./GTEA61_val\")\n",
        "  os.makedirs(\"./GTEA61_val/flow_x_processed\")\n",
        "  os.makedirs(\"./GTEA61_val/flow_y_processed\")\n",
        "  os.makedirs(\"./GTEA61_val/processed_frames2\")\n",
        "  pass\n",
        "\n",
        "\n",
        "for directory in sorted(os.listdir(base_dataset)):\n",
        "  if not directory.startswith('.'):\n",
        "    directory_path = os.path.join(base_dataset, directory)\n",
        "    new_directory_path = os.path.join(\"./GTEA61_val\", directory)\n",
        "\n",
        "    for user in sorted(os.listdir(directory_path)):\n",
        "      if not user.startswith('.') and user == \"S2\":\n",
        "        current_folder = os.path.join(directory_path, user)\n",
        "        !cp -r \"{current_folder}\" \"{new_directory_path}\"\n",
        "        !rm -rf \"{current_folder}\"\n",
        "        print(current_folder)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./GTEA61/flow_x_processed/S2\n",
            "./GTEA61/flow_y_processed/S2\n",
            "./GTEA61/processed_frames2/S2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nepLRofOQ0ji"
      },
      "source": [
        "# Se scritti uguali ai parametri della funzione può venire None come risultato\n",
        "# per questo SEQLEN è maiuscolo \n",
        "SEQLEN = 7\n",
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "num_classes = 61\n",
        "#STAGE = None\n",
        "train_data_dir = \"./GTEA61\"\n",
        "val_data_dir = \"./GTEA61_val\"\n",
        "#stage1_dict = None\n",
        "out_dir = \".out_dir\"\n",
        "trainBatchSize = 32\n",
        "valBatchSize = 64\n",
        "numEpochs1 = 3\n",
        "numEpochs2 = 3\n",
        "lr1 = 1e-3\n",
        "lr2 = 1e-4\n",
        "decay_factor = 0.1\n",
        "decay_step1 = [25, 75, 150]\n",
        "decay_step2 = [25, 75]\n",
        "#Boh?!\n",
        "memSize = 512\n",
        "alpha=1"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZoYPO1hQ0o5"
      },
      "source": [
        "#!rm -rf \"ML_DL_Project\"\n",
        "#!rm -rf \"./GTEA61\"\n",
        "#!rm -rf \"./GTEA61_val\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrGDU3ojQ0uu"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "# This is without attention, we must address this better\n",
        "#from ML_DL_Project.Scripts.convLSTMmodel import *\n",
        "from ML_DL_Project.Scripts.SelfSupObjectAttentionModelConvLSTM import *\n",
        "from ML_DL_Project.Scripts.RegObjectAttentionModelConvLSTM import *\n",
        "from ML_DL_Project.Scripts.spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
        "                                RandomHorizontalFlip)\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from ML_DL_Project.Scripts.resnetMod import *\n",
        "#from ML_DL_Project.Scripts.makeDatasetRGB import *\n",
        "from ML_DL_Project.Scripts.makeMmaps import *\n",
        "#Prende il makeDataset dell'ultimo script importato\n",
        "import argparse\n",
        "import sys"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec7hvOY2UjYG"
      },
      "source": [
        "!rm -rf \"./out_dir\""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLWIt6B1RvRH"
      },
      "source": [
        "   model_folder = os.path.join('./', \"out_dir\", \"./GTEA61\", 'ss')\n",
        "   if os.path.exists(model_folder):\n",
        "        print('Directory {} exists!'.format(model_folder))\n",
        "        sys.exit()\n",
        "   os.makedirs(model_folder)\n",
        "    \n",
        "    # Log files\n",
        "   writer = SummaryWriter(model_folder)\n",
        "   train_log_loss = open((model_folder + '/train_log_loss.txt'), 'w')\n",
        "   train_log_acc = open((model_folder + '/train_log_acc.txt'), 'w')\n",
        "   val_log_loss = open((model_folder + '/val_log_loss.txt'), 'w')\n",
        "   val_log_acc = open((model_folder + '/val_log_acc.txt'), 'w')\n",
        "\n",
        "    # Data loader\n",
        "   normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "   spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                                 ToTensor(), normalize])\n",
        "\n",
        "   vid_seq_train = makeDataset(train_data_dir, spatial_transform=spatial_transform, seqLen=SEQLEN, fmt='.png')\n",
        "\n",
        "   trainInstances = vid_seq_train.__len__()\n",
        "\n",
        "   train_loader = torch.utils.data.DataLoader(vid_seq_train, batch_size=trainBatchSize,\n",
        "                            shuffle=True, num_workers=4, pin_memory=True)\n",
        "    \n",
        "   if val_data_dir is not None:\n",
        "       vid_seq_val = makeDataset(val_data_dir,spatial_transform = Compose([Scale(256),\n",
        "                                                                    CenterCrop(224),\n",
        "                                                                    ToTensor(),\n",
        "                                                                    normalize]),\n",
        "                            seqLen=SEQLEN, fmt='.png')\n",
        "       valInstances = vid_seq_val.__len__()\n",
        "\n",
        "       val_loader = torch.utils.data.DataLoader(vid_seq_val, batch_size=valBatchSize,\n",
        "                                shuffle=False, num_workers=2, pin_memory=True)\n",
        "    \n",
        "   "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub1VBiPCWyNl"
      },
      "source": [
        "def train(stage, learning_rate, numEpochs, decay_step, stage1_dict=None):\n",
        "\n",
        "   train_params = []\n",
        "   if stage == 1:\n",
        "       model = SelfSupAttentionModel(num_classes=num_classes, mem_size=memSize)\n",
        "       model.train(False)\n",
        "       for params in model.parameters():\n",
        "\n",
        "           params.requires_grad = False\n",
        "   else: # stage == 2\n",
        "       model = SelfSupAttentionModel(num_classes=num_classes, mem_size=memSize)\n",
        "       model.load_state_dict(torch.load(stage1_dict),strict=False)\n",
        "       model.train(False)\n",
        "        \n",
        "       for params in model.parameters():\n",
        "           params.requires_grad = False\n",
        "        #\n",
        "       for params in model.resNet.layer4[0].conv1.parameters():\n",
        "           params.requires_grad = True\n",
        "           train_params += [params]\n",
        "\n",
        "       for params in model.resNet.layer4[0].conv2.parameters():\n",
        "           params.requires_grad = True\n",
        "           train_params += [params]\n",
        "\n",
        "       for params in model.resNet.layer4[1].conv1.parameters():\n",
        "           params.requires_grad = True\n",
        "           train_params += [params]\n",
        "\n",
        "       for params in model.resNet.layer4[1].conv2.parameters():\n",
        "           params.requires_grad = True\n",
        "           train_params += [params]\n",
        "\n",
        "       for params in model.resNet.layer4[2].conv1.parameters():\n",
        "           params.requires_grad = True\n",
        "           train_params += [params]\n",
        "        \n",
        "       for params in model.resNet.layer4[2].conv2.parameters():\n",
        "           params.requires_grad = True\n",
        "           train_params += [params]\n",
        "        #\n",
        "       for params in model.resNet.fc.parameters():\n",
        "           params.requires_grad = True\n",
        "           train_params += [params]\n",
        "\n",
        "       model.resNet.layer4[0].conv1.train(True)\n",
        "       model.resNet.layer4[0].conv2.train(True)\n",
        "       model.resNet.layer4[1].conv1.train(True)\n",
        "       model.resNet.layer4[1].conv2.train(True)\n",
        "       model.resNet.layer4[2].conv1.train(True)\n",
        "       model.resNet.layer4[2].conv2.train(True)\n",
        "       model.resNet.fc.train(True)\n",
        "    \n",
        "   for params in model.lstm_cell.parameters():\n",
        "       params.requires_grad = True\n",
        "       train_params += [params]\n",
        "\n",
        "   for params in model.classifier.parameters():\n",
        "       params.requires_grad = True\n",
        "       train_params += [params]\n",
        "\n",
        "   model.lstm_cell.train(True)\n",
        "\n",
        "   model.classifier.train(True)\n",
        "   model.cuda()\n",
        "\n",
        "   loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "   optimizer_fn = torch.optim.Adam(train_params, lr=lr1, weight_decay=4e-5, eps=1e-4)\n",
        "\n",
        "   optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=decay_step,\n",
        "                                                           gamma=decay_factor)\n",
        "\n",
        "   train_iter = 0\n",
        "   min_accuracy = 0\n",
        "   for epoch in range(numEpochs):\n",
        "       epoch_loss = 0\n",
        "       numCorrTrain = 0\n",
        "       mmap_loss = 0\n",
        "       trainSamples = 0\n",
        "       iterPerEpoch = 0\n",
        "        \n",
        "       model.lstm_cell.train(True)\n",
        "       model.classifier.train(True)\n",
        "       writer.add_scalar('lr', optimizer_fn.param_groups[0]['lr'], epoch+1)\n",
        "        \n",
        "       if stage == 2:\n",
        "           model.resNet.layer4[0].conv1.train(True)\n",
        "           model.resNet.layer4[0].conv2.train(True)\n",
        "           model.resNet.layer4[1].conv1.train(True)\n",
        "           model.resNet.layer4[1].conv2.train(True)\n",
        "           model.resNet.layer4[2].conv1.train(True)\n",
        "           model.resNet.layer4[2].conv2.train(True)\n",
        "           model.resNet.fc.train(True)\n",
        "        \n",
        "        #for i, (inputs, targets) in enumerate(train_loader):\n",
        "       for inputs, inputMmap, targets in train_loader:\n",
        "           train_iter += 1\n",
        "           iterPerEpoch += 1\n",
        "           optimizer_fn.zero_grad()\n",
        "\n",
        "           inputMmap = inputMmap.cuda()\n",
        "            \n",
        "           inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).cuda())\n",
        "           labelVariable = Variable(targets.cuda())\n",
        "           trainSamples += inputs.size(0)\n",
        "            \n",
        "           output_label, _ , mmapPrediction = model(inputVariable)\n",
        "\n",
        "           mmapPrediction = mmapPrediction.view(-1,2)         \n",
        "           inputMmap = torch.reshape(inputMmap, (-1,)) #.long()\n",
        "           inputMmap = torch.round(inputMmap).long() #making things black and white again\n",
        "            \n",
        "\n",
        "           loss2 = alpha*loss_fn(mmapPrediction,inputMmap)\n",
        "           loss = loss_fn(output_label, labelVariable)\n",
        "\n",
        "            #Weighting the loss of the ss task\n",
        "            #by multiplying it by alpha\n",
        "           total_loss = loss  + loss2\n",
        "           total_loss.backward()\n",
        "            \n",
        "           optimizer_fn.step()\n",
        "           _, predicted = torch.max(output_label.data, 1)\n",
        "           numCorrTrain += (predicted == targets.cuda()).sum()\n",
        "           mmap_loss += loss2.item()\n",
        "           epoch_loss += loss.item()\n",
        "        \n",
        "       optim_scheduler.step()\n",
        "       avg_mmap_loss = mmap_loss / iterPerEpoch\n",
        "       avg_loss = epoch_loss/iterPerEpoch\n",
        "       trainAccuracy = torch.true_divide(numCorrTrain , trainSamples) * 100\n",
        "\n",
        "       print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch+1, avg_loss, trainAccuracy))\n",
        "       print('Mmap loss after {} epoch = {}% '.format(epoch + 1, avg_mmap_loss))\n",
        "\n",
        "       writer.add_scalar('train/epoch_loss', avg_loss, epoch+1)\n",
        "       writer.add_scalar('train/accuracy', trainAccuracy, epoch+1)\n",
        "       writer.add_scalar('mmap_train_loss',avg_mmap_loss,epoch+1)\n",
        "\n",
        "       train_log_loss.write('Training mmap loss after {} epoch= {}'.format(epoch+1,avg_mmap_loss))\n",
        "       train_log_loss.write('Training loss after {} epoch = {}\\n'.format(epoch+1, avg_loss))\n",
        "       train_log_acc.write('Training accuracy after {} epoch = {}\\n'.format(epoch+1, trainAccuracy))\n",
        "        \n",
        "       if valDatasetDir is not None:\n",
        "           model.train(False)\n",
        "           val_loss_epoch = 0\n",
        "           val_iter = 0\n",
        "           val_mmap_loss = 0\n",
        "           val_samples = 0\n",
        "           numCorr = 0\n",
        "           mmap_loss = 0\n",
        "\n",
        "           with torch.no_grad():\n",
        "                #for j, (inputs, targets) in enumerate(val_loader):\n",
        "               for inputs, inputMmap, targets in val_loader:\n",
        "                   val_iter += 1\n",
        "                   val_samples += inputs.size(0)\n",
        "                   inputMmap = inputMmap.cuda()\n",
        "\n",
        "                    \n",
        "                   inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).cuda())\n",
        "                   labelVariable = Variable(targets.cuda(async=True))\n",
        "                    #labelVariable = Variable(targets.cuda())\n",
        "                    \n",
        "                   output_label, _ , mmapPrediction = model(inputVariable)\n",
        "\n",
        "                   mmapPrediction = mmapPrediction.view(-1,2)\n",
        "                   inputMmap = torch.reshape(inputMmap, (-1,)) #.long()\n",
        "                   inputMmap = torch.round(inputMmap).long()\n",
        "                   loss2 = alpha*loss_fn(mmapPrediction,inputMmap)\n",
        "\n",
        "                   val_loss = loss_fn(output_label, labelVariable)\n",
        "                   val_loss_epoch += val_loss.item()\n",
        "                   val_mmap_loss += loss2.item()\n",
        "                    \n",
        "                   _, predicted = torch.max(output_label.data, 1)\n",
        "                   numCorr += (predicted == targets.cuda()).sum()\n",
        "\n",
        "           avg_mmap_val_loss = val_mmap_loss / val_iter\n",
        "           val_accuracy = torch.true_divide(numCorr , val_samples) * 100\n",
        "           avg_val_loss = val_loss_epoch / val_iter\n",
        "            \n",
        "           print('Val MMap Loss after {} epochs, loss = {}'.format(epoch + 1, avg_mmap_val_loss))\n",
        "           print('Val: Epoch = {} | Loss {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "           writer.add_scalar('val mmap/epoch_loss', avg_mmap_val_loss, epoch + 1)\n",
        "           writer.add_scalar('val/epoch_loss', avg_val_loss, epoch + 1)\n",
        "           writer.add_scalar('val/accuracy', val_accuracy, epoch + 1)\n",
        "           val_log_loss.write('Val MMap Loss after {} epochs = {}\\n'.format(epoch + 1, avg_mmap_val_loss))\n",
        "           val_log_loss.write('Val Loss after {} epochs = {}\\n'.format(epoch + 1, avg_val_loss))\n",
        "           val_log_acc.write('Val Accuracy after {} epochs = {}%\\n'.format(epoch + 1, val_accuracy))\n",
        "            \n",
        "           if val_accuracy > min_accuracy:\n",
        "               save_path_model = (model_folder + '/model_rgb_state_dict.pth')\n",
        "               torch.save(model.state_dict(), save_path_model)\n",
        "               min_accuracy = val_accuracy\n",
        "\n",
        "   train_log_loss.close()\n",
        "   train_log_acc.close()\n",
        "   val_log_acc.close()\n",
        "   val_log_loss.close()\n",
        "   writer.export_scalars_to_json(model_folder + \"/all_scalars.json\")\n",
        "   writer.close()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrI6XignXD53",
        "outputId": "491eca2e-efbb-4507-9f84-e7c7e97bce59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "train(1, lr1, numEpochs1, decay_step1)\n",
        "\n",
        "train_log_loss.close()\n",
        "train_log_acc.close()\n",
        "val_log_acc.close()\n",
        "val_log_loss.close()\n",
        "#writer.export_scalars_to_json(model_folder + \"/all_scalars.json\")\n",
        "writer.flush()\n",
        "writer.close()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-163b2b67c07d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumEpochs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_step1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_log_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_log_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_log_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-39aada77f329>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(stage, learning_rate, numEpochs, decay_step, stage1_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m            \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmapPrediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputMmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 948\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 2216\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   2217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (5488) to match target batch_size (11239424)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFcitTYeRxuI"
      },
      "source": [
        "writer = SummaryWriter('runs/exp1/stage/2')\n",
        "train_log_loss = open((model_folder + '/train_log_loss.txt'), 'w')\n",
        "train_log_acc = open((model_folder + '/train_log_acc.txt'), 'w')\n",
        "val_log_loss = open((model_folder + '/val_log_loss.txt'), 'w')\n",
        "val_log_acc = open((model_folder + '/val_log_acc.txt'), 'w')\n",
        "\n",
        "\n",
        "train(2, lr2, numEpochs2, decay_step2, stage1_dict=\"./out_dir/GTEA61/rgb/model_rgb_state_dict.pth\")\n",
        "\n",
        "train_log_loss.close()\n",
        "train_log_acc.close()\n",
        "val_log_acc.close()\n",
        "val_log_loss.close()\n",
        "#writer.export_scalars_to_json(model_folder + \"/all_scalars.json\")\n",
        "writer.flush()\n",
        "writer.close()\n",
        "\n",
        "%tensorboard --logdir=runs/exp1\n",
        "#!rm -rf ./runs/exp1/stage\n",
        "#Using it only if run again, in view to clean the folders"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}